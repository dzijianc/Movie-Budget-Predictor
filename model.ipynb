{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, cuda\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'val':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieAttributeDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path)\n",
    "        budget = self.img_labels.iloc[idx, 1]\n",
    "        genres = torch.Tensor(ast.literal_eval(self.img_labels.iloc[idx, 2]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, budget, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"training_data.csv\")\n",
    "train_set = training_data.sample(frac=0.9)\n",
    "val_set = training_data.loc[~training_data.index.isin(train_set.index)]\n",
    "# print(train_set)\n",
    "# print(val_set)\n",
    "train_set.to_csv(\"train_set.csv\", index=False)\n",
    "val_set.to_csv(\"val_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieAttributeDataset(annotations_file=\"train_set.csv\", img_dir=\"\", transform=image_transforms[\"train\"])\n",
    "val_dataset = MovieAttributeDataset(annotations_file=\"val_set.csv\", img_dir=\"\", transform=image_transforms[\"val\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 23])\n"
     ]
    }
   ],
   "source": [
    "for images, budgets, genres in train_dataloader:\n",
    "    print(images.shape) # (batch_size, color_channels, height, width)\n",
    "    print(budgets.shape)\n",
    "    print(genres.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # m x m x 3 image\n",
    "        # 224x224x3 => 222x222x32, kernel: k -> m - k + 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.d1 = nn.Linear(222 * 222 * 32, 256)\n",
    "        self.d2 = nn.Linear(256, 256)\n",
    "        self.d3 = nn.Linear(256, 256)\n",
    "        self.d4 = nn.Linear(256, 5)\n",
    "\n",
    "        self.d5 = nn.Linear(256, 23)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 64x3x224x224 => 32x32x222x222\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # flatten => 64 x (32*222*222)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "\n",
    "        # 64 x (32*222*222) => 64x256\n",
    "        x = self.d1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "\n",
    "        # 64 x 256 => 64x256\n",
    "        x = self.d2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "\n",
    "        # 64 x 256 => 64x256\n",
    "        x = self.d3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "\n",
    "        # logits (the raw, unnormalized predictions) => 64x5\n",
    "        out = self.d4(x)\n",
    "        # out = F.softmax(logits, dim=1) # turn logits into a set of probabilities.\n",
    "\n",
    "        out2 = self.d5(x)\n",
    "        # out2 = F.sigmoid(logits2, dim=1)\n",
    "        return out, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MyModel()\n",
    "# for images, labels in train_dataloader:\n",
    "#     print(\"batch size:\", images.shape)\n",
    "#     out = model(images)\n",
    "#     print(out.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyModel()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.BCEWithLogitsLoss(reduction='mean') # binary cross entropy loss + softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          criterion1,\n",
    "          criterion2,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          save_file_name,\n",
    "          max_epochs_stop=3,\n",
    "          n_epochs=20,\n",
    "          print_every=2):\n",
    "    \"\"\"Train a PyTorch Model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): cnn to train\n",
    "        criterion (PyTorch loss): objective to minimize\n",
    "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
    "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "        n_epochs (int): maximum number of training epochs\n",
    "        print_every (int): frequency of epochs to print training stats\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        model (PyTorch model): trained cnn with best weights\n",
    "        history (DataFrame): history of train and validation loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    valid_max_acc = 0\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = timer()\n",
    "\n",
    "    # Main loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # keep track of training and validation loss each epoch\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_budget_acc = 0\n",
    "        train_genre_acc = 0\n",
    "        valid_budget_acc = 0\n",
    "        valid_genre_acc = 0\n",
    "\n",
    "        # Set to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "\n",
    "        ## training step\n",
    "        for ii, (images, budgets, genres) in enumerate(train_dataloader):\n",
    "\n",
    "            images = images.to(device)\n",
    "            budgets = budgets.to(device)\n",
    "            genres = genres.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Predicted outputs are non-log probabilities\n",
    "            budget_pred, genre_pred = model(images)\n",
    "            # 64 x 23\n",
    "\n",
    "            # Loss and backpropagation of gradients\n",
    "            loss1 = criterion1(budget_pred, budgets)\n",
    "            loss2 = criterion2(genre_pred, genres)\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Calculate accuracy by finding max log probability\n",
    "            _, pred1 = torch.max(budget_pred, dim=1)\n",
    "            correct_tensor = pred1.eq(budgets.data.view_as(pred1))\n",
    "\n",
    "            _, idx = genre_pred.topk(3, dim=1)\n",
    "            pred2 = torch.zeros_like(genre_pred)\n",
    "            pred2[torch.arange(genres.size(0)).unsqueeze(1), idx] = 1\n",
    "            correct_tensor2 = pred2.eq(genres.data.view_as(pred2))\n",
    "            \n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            accuracy2 = torch.mean(correct_tensor2.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            train_budget_acc += accuracy.item() * images.size(0)\n",
    "            train_genre_acc += accuracy2.item() * genres.size(0)\n",
    "\n",
    "            # Track training progress\n",
    "            print(\n",
    "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "                end='\\r')\n",
    "\n",
    "        # After training loops ends, start validation\n",
    "        else:\n",
    "            model.epochs += 1\n",
    "\n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Validation loop\n",
    "                for images, budgets, genres in valid_loader:\n",
    "                    # Tensors to gpu\n",
    "                    images = images.to(device)\n",
    "                    budgets = budgets.to(device)\n",
    "                    genres = genres.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    budget_pred, genre_pred = model(images)\n",
    "\n",
    "                    # Validation loss\n",
    "                    loss = criterion(budget_pred, budgets)\n",
    "                    # Multiply average loss times the number of examples in batch\n",
    "                    valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    _, pred = torch.max(budget_pred, dim=1)\n",
    "                    correct_tensor = pred.eq(budgets.data.view_as(pred))\n",
    "\n",
    "                    _, idx = genre_pred.topk(3, dim=1)\n",
    "                    pred2 = torch.zeros_like(genre_pred)\n",
    "                    pred2[torch.arange(genres.size(0)).unsqueeze(1), idx] = 1\n",
    "                    correct_tensor2 = pred2.eq(genres.data.view_as(pred2))\n",
    "\n",
    "                    accuracy = torch.mean(\n",
    "                        correct_tensor.type(torch.FloatTensor))\n",
    "                    accuracy2 = torch.mean(correct_tensor2.type(torch.FloatTensor))\n",
    "                    # Multiply average accuracy times the number of examples\n",
    "                    valid_budget_acc += accuracy.item() * images.size(0)\n",
    "                    valid_genre_acc += accuracy2.item() * genres.size(0)\n",
    "\n",
    "                # Calculate average losses\n",
    "                train_loss = train_loss / len(train_loader.dataset)\n",
    "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "                # Calculate average accuracy\n",
    "                train_budget_acc = train_budget_acc / len(train_loader.dataset)\n",
    "                train_genre_acc = train_genre_acc / len(train_loader.dataset)\n",
    "                valid_budget_acc = valid_budget_acc / len(valid_loader.dataset)\n",
    "                valid_genre_acc = valid_genre_acc / len(valid_loader.dataset)\n",
    "\n",
    "                history.append([train_loss, valid_loss, train_budget_acc, valid_budget_acc, train_genre_acc, valid_genre_acc])\n",
    "\n",
    "                # Print training and validation results\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\n",
    "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                    )\n",
    "                    print(\n",
    "                        f'\\t\\tBudget Training Accuracy: {100 * train_budget_acc:.2f}%\\t Budget Validation Accuracy: {100 * valid_budget_acc:.2f}%'\n",
    "                    )\n",
    "                    print(\n",
    "                        f'\\t\\tGenre Training Accuracy: {100 * train_genre_acc:.2f}%\\t Genre Validation Accuracy: {100 * valid_genre_acc:.2f}%'\n",
    "                    )\n",
    "\n",
    "                # Save the model if validation loss decreases\n",
    "                if valid_loss < valid_loss_min:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), save_file_name)\n",
    "                    # Track improvement\n",
    "                    epochs_no_improve = 0\n",
    "                    valid_loss_min = valid_loss\n",
    "                    valid_best_acc = valid_budget_acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                # Otherwise increment count of epochs with no improvement\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    # Trigger early stopping\n",
    "                    if epochs_no_improve >= max_epochs_stop:\n",
    "                        print(\n",
    "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_best_acc:.2f}%'\n",
    "                        )\n",
    "                        total_time = timer() - overall_start\n",
    "                        print(\n",
    "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
    "                        )\n",
    "\n",
    "                        # Load the best state dict\n",
    "                        model.load_state_dict(torch.load(save_file_name))\n",
    "                        # Attach the optimizer\n",
    "                        model.optimizer = optimizer\n",
    "\n",
    "                        # Format history\n",
    "                        history = pd.DataFrame(\n",
    "                            history,\n",
    "                            columns=[\n",
    "                                'train_loss', 'valid_loss', 'train_budget_acc', \n",
    "                                'valid_budget_acc', 'train_genre_acc', 'valid_genre_acc'\n",
    "                            ])\n",
    "                        return model, history\n",
    "\n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    # Record overall time and print out stats\n",
    "    total_time = timer() - overall_start\n",
    "    print(\n",
    "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_best_acc:.2f}%'\n",
    "    )\n",
    "    print(\n",
    "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
    "    )\n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss', 'train_budget_acc', 'valid_budget_acc', 'train_genre_acc', 'valid_genre_acc'])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training from Scratch.\n",
      "\n",
      "Epoch: 1\t100.00% complete. 45.97 seconds elapsed in epoch.\n",
      "Epoch: 1 \tTraining Loss: 1.9756 \tValidation Loss: 1.3319\n",
      "\t\tBudget Training Accuracy: 56.40%\t Budget Validation Accuracy: 50.60%\n",
      "\t\tGenre Training Accuracy: 86.01%\t Genre Validation Accuracy: 84.78%\n",
      "Epoch: 3\t100.00% complete. 45.39 seconds elapsed in epoch.\n",
      "Epoch: 3 \tTraining Loss: 1.4991 \tValidation Loss: 1.3007\n",
      "\t\tBudget Training Accuracy: 56.60%\t Budget Validation Accuracy: 50.60%\n",
      "\t\tGenre Training Accuracy: 86.08%\t Genre Validation Accuracy: 84.68%\n",
      "Epoch: 5\t100.00% complete. 49.91 seconds elapsed in epoch.\n",
      "Epoch: 5 \tTraining Loss: 1.4907 \tValidation Loss: 1.2919\n",
      "\t\tBudget Training Accuracy: 56.66%\t Budget Validation Accuracy: 50.60%\n",
      "\t\tGenre Training Accuracy: 86.20%\t Genre Validation Accuracy: 84.78%\n",
      "Epoch: 7\t100.00% complete. 49.39 seconds elapsed in epoch.\n",
      "Epoch: 7 \tTraining Loss: 1.4928 \tValidation Loss: 1.2880\n",
      "\t\tBudget Training Accuracy: 56.66%\t Budget Validation Accuracy: 50.60%\n",
      "\t\tGenre Training Accuracy: 86.21%\t Genre Validation Accuracy: 84.94%\n",
      "Epoch: 9\t100.00% complete. 49.30 seconds elapsed in epoch.\n",
      "Epoch: 9 \tTraining Loss: 1.5376 \tValidation Loss: 1.2871\n",
      "\t\tBudget Training Accuracy: 56.73%\t Budget Validation Accuracy: 50.60%\n",
      "\t\tGenre Training Accuracy: 86.21%\t Genre Validation Accuracy: 84.94%\n",
      "\n",
      "Best epoch: 9 with loss: 1.29 and acc: 50.60%\n",
      "530.60 total seconds elapsed. 58.96 seconds per epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MyModel(\n",
       "   (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "   (d1): Linear(in_features=1577088, out_features=256, bias=True)\n",
       "   (d2): Linear(in_features=256, out_features=256, bias=True)\n",
       "   (d3): Linear(in_features=256, out_features=256, bias=True)\n",
       "   (d4): Linear(in_features=256, out_features=5, bias=True)\n",
       "   (d5): Linear(in_features=256, out_features=23, bias=True)\n",
       " ),\n",
       "    train_loss  valid_loss  train_budget_acc  valid_budget_acc  \\\n",
       " 0  224.895041    1.530242          0.385224          0.500000   \n",
       " 1    1.975552    1.331917          0.563984          0.505952   \n",
       " 2    1.547209    1.301177          0.566623          0.505952   \n",
       " 3    1.499132    1.300748          0.565963          0.505952   \n",
       " 4    1.510030    1.289033          0.563325          0.505952   \n",
       " 5    1.490667    1.291912          0.566623          0.505952   \n",
       " 6    1.501748    1.291521          0.566623          0.505952   \n",
       " 7    1.492839    1.287957          0.566623          0.505952   \n",
       " 8    1.487282    1.511417          0.566623          0.500000   \n",
       " 9    1.537575    1.287085          0.567282          0.505952   \n",
       " \n",
       "    train_genre_acc  valid_genre_acc  \n",
       " 0         0.824710         0.849896  \n",
       " 1         0.860101         0.847826  \n",
       " 2         0.861535         0.850414  \n",
       " 3         0.860847         0.846791  \n",
       " 4         0.861478         0.849379  \n",
       " 5         0.861994         0.847826  \n",
       " 6         0.862453         0.849379  \n",
       " 7         0.862109         0.849379  \n",
       " 8         0.861707         0.849379  \n",
       " 9         0.862109         0.849379  )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, criterion, criterion2, optimizer, train_dataloader, val_dataloader, \"test.pt\", 3, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MovieAttributeDataset(annotations_file=\"test_data.csv\", img_dir=\"\", transform=image_transforms[\"test\"])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = MyModel()\n",
    "final.load_state_dict(torch.load('test.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader):\n",
    "    budget_acc = 0\n",
    "    genre_acc = 0\n",
    "    for (images, budgets, genres) in test_dataloader:\n",
    "        out1, out2 = model(images)\n",
    "\n",
    "        budget_pred = torch.argmax(out1, dim=1)\n",
    "        correct = budget_pred.eq(budgets.data.view_as(budget_pred))\n",
    "\n",
    "        _, idx = out2.topk(3, dim=1)\n",
    "        genre_pred = torch.zeros_like(out2)\n",
    "        genre_pred[torch.arange(genres.size(0)).unsqueeze(1), idx] = 1\n",
    "        correct2 = genre_pred.eq(genres.data.view_as(genre_pred))\n",
    "        # Need to convert correct tensor from int to float to average\n",
    "        budget_accuracy = torch.mean(correct.type(torch.FloatTensor))\n",
    "        genre_accuracy = torch.mean(correct2.type(torch.FloatTensor))\n",
    "        # Multiply average accuracy times the number of examples in batch\n",
    "        budget_acc += budget_accuracy.item() * images.size(0)\n",
    "        genre_acc += genre_accuracy.item() * genres.size(0)\n",
    "    \n",
    "    budget_acc = budget_acc / len(test_dataloader.dataset)\n",
    "    genre_acc = genre_acc / len(test_dataloader.dataset)\n",
    "    print(\n",
    "        f'Budget Accuracy: {100 * budget_acc:.2f}%\\t Genre Accuracy: {100 * genre_acc:.2f}%'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget Accuracy: 100.00%\t Genre Accuracy: 85.60%\n"
     ]
    }
   ],
   "source": [
    "test(final, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
